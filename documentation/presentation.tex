\documentclass[Serif, 10pt, brown]{beamer}
\usepackage{booktabs,xcolor}
%\usepackage[svgnames,table]{xcolor}
%\usepackage[tableposition=above]{caption}
\usepackage{pifont}
\newcommand*\CHECK{\ding{51}}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
%
\usepackage{setspace,mathtools,amssymb,multirow,array,amsmath,tikz}
\usepackage[normalsize]{subfigure}
\usetikzlibrary{patterns}
\usetikzlibrary{automata,positioning,decorations.pathreplacing,decorations}

\usepackage{curves}
\usepackage{wasysym}
\usepackage{epsfig,epstopdf,graphicx}

\curvewarnfalse
%
\newtheorem{proposition}{Proposition}
\theoremstyle{example}
\newtheorem{theoremh}{Theorem}
\theoremstyle{plain}
\renewcommand{\textfraction}{0.01}
\renewcommand{\floatpagefraction}{0.99}
\newcommand{\ul}{\underline}
\newcounter{units}
%
\usepackage[round]{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%
%
\setbeamercovered{dynamic}
% Logo
\logo{\includegraphics[width=0.5in,keepaspectratio]{iitb_logo.png}}
%
% Setup
\mode<presentation>
	{
\usetheme[right,currentsection, hideothersubsections]{UTD}
			\useoutertheme{sidebar} \useinnertheme[shadow]{rounded}
			\usecolortheme{whale} \usecolortheme{orchid}
			\usefonttheme[onlymath]{serif}
			\setbeamertemplate{footline}{\centerline{Slide \insertframenumber/\inserttotalframenumber}}
	}
%
% Title
\usebeamercolor[fg]{author in sidebar}
\title[{Approximate Nearest Neighbor Search via Group Testing}]{\sc Approximate Nearest Neighbor Search via Group Testing}
\author[\ul{Authors}]{
  \begin{tabular}{c c c}
    {\bf Saksham Rathi} & {\bf Kshitij Vaidya} & {\bf Ekansh Ravi Shankar} \\
    (22B1003) & (22B1829) & (22B1032)
  \end{tabular}
}
\institute[UTD]{\sc\small CS754: Advanced Image Processing\\ Under Prof. Ajit Rajwade}
\date[UCI]{Indian Institute of Technology Bombay \\ Spring 2025}
%
%Presentation
\begin{document}
\frame{\titlepage}
%
%
%Slides

%TOC

\begin{frame}
	\transblindsvertical
	\frametitle{Contents}
	\tableofcontents[hidesubsections]
\end{frame}
\note[itemize]{
\item Here's the overall structure of my talk today.
}

\section{Introduction}
\begin{frame}{Nearest Neighbor Search}
	\begin{itemize}
		\item Nearest neighbor search is a fundamental problem with many applications in machine learning systems.
		\item {\bf Task:} Given a dataset $D = \{x_1, x_2, \dots , x_N\}$, the goal is to build a data structure that can be queried with any point $q$ to obtain a small set of points $x_i \in D$ that have high similarity (low distance) to the query. This structure is called an index.
		\item Such tasks frequently arise in genomics, web-scale data mining,
		machine learning, and other large-scale applications.
	\end{itemize}
\end{frame}

\begin{frame}{Locality Sensitive Hashing}
	\begin{itemize}
		\item {\bf Locality Sensitive Hashing (LSH)} algorithms use an LSH function to partition the dataset into buckets.
		\item The hash function is selected so that the distance between points in the same bucket is likely to be small. 
		\item To find the near neighbors of a query, we hash the query and compute the distance to every point in the corresponding bucket.
		\item  {\bf Count-Based LSH} identifies neighbors by simply counting how many times two points land in the same hash bucket across multiple hash functions.
	\end{itemize}
\end{frame}

\begin{frame}{Formal Problem Statement}
	\begin{itemize}
		\item \textbf{(R, c)-Approximate Near Neighbor:} Given a dataset $D$, if there exists a point within distance $R$ of a query $y$, return some point within distance $c \cdot R$, with high probability.
		\begin{itemize}
			\item $R$ is the distance threshold (radius).
			\item $c > 1$ is the approximation factor.
		\end{itemize}
		\item Any algorithm that solves the randomized nearest neighbor problem also solves the approximate near neighbor problem with $c=1$ and any $R \ge$ distance to the nearest neighbor.
		\item {(Definition)}
		{\bf Randomized Nearest neighbor:} Given a dataset $D$ and a distance metric $d(\cdot, \cdot)$ and a failure probability $\delta \in [0, 1]$, construct a data structure which, given a query point $y$ reports the point $x \in D$ with the smallest distance $d(x,y)$ with probability greater than $1 - \delta$.
	\end{itemize}
\end{frame}





\end{document}


