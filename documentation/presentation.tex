\documentclass[Serif, 10pt, brown]{beamer}
\usepackage{booktabs,xcolor}
%\usepackage[svgnames,table]{xcolor}
%\usepackage[tableposition=above]{caption}
\usepackage{pifont}
\newcommand*\CHECK{\ding{51}}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
%
\usepackage{setspace,mathtools,amssymb,multirow,array,amsmath,tikz}
\usepackage[normalsize]{subfigure}
\usetikzlibrary{patterns}
\usetikzlibrary{automata,positioning,decorations.pathreplacing,decorations}

\usepackage{curves}
\usepackage{wasysym}
\usepackage{epsfig,epstopdf,graphicx}

\curvewarnfalse
%
\newtheorem{proposition}{Proposition}
\theoremstyle{example}
\newtheorem{theoremh}{Theorem}
\theoremstyle{plain}
\renewcommand{\textfraction}{0.01}
\renewcommand{\floatpagefraction}{0.99}
\newcommand{\ul}{\underline}
\newcounter{units}
%
\usepackage[round]{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%
%
\setbeamercovered{dynamic}
% Logo
\logo{\includegraphics[width=0.5in,keepaspectratio]{iitb_logo.png}}
%
% Setup
\mode<presentation>
	{
\usetheme[right,currentsection, hideothersubsections]{UTD}
			\useoutertheme{sidebar} \useinnertheme[shadow]{rounded}
			\usecolortheme{whale} \usecolortheme{orchid}
			\usefonttheme[onlymath]{serif}
			\setbeamertemplate{footline}{\centerline{Slide \insertframenumber/\inserttotalframenumber}}
	}
%
% Title
\usebeamercolor[fg]{author in sidebar}
\title[{Approximate Nearest Neighbor Search via Group Testing}]{\sc Approximate Nearest Neighbor Search via Group Testing}
\author[\ul{Authors}]{
  \begin{tabular}{c c c}
    {\bf Saksham Rathi} & {\bf Kshitij Vaidya} & {\bf Ekansh Ravi Shankar} \\
    (22B1003) & (22B1829) & (22B1032)
  \end{tabular}
}
\institute[UTD]{\sc\small CS754: Advanced Image Processing\\ Under Prof. Ajit Rajwade}
\date[UCI]{Indian Institute of Technology Bombay \\ Spring 2025}
%
%Presentation
\begin{document}
\frame{\titlepage}
%
%
%Slides

%TOC

\begin{frame}
	\transblindsvertical
	\frametitle{Contents}
	\tableofcontents[hidesubsections]
\end{frame}
\note[itemize]{
\item Here's the overall structure of my talk today.
}

\section{Introduction}
\begin{frame}{Nearest Neighbor Search}
	\begin{itemize}
		\item Nearest neighbor search is a fundamental problem with many applications in machine learning systems.
		\item {\bf Task:} Given a dataset $D = \{x_1, x_2, \dots , x_N\}$, the goal is to build a data structure that can be queried with any point $q$ to obtain a small set of points $x_i \in D$ that have high similarity (low distance) to the query. This structure is called an index.
		\item Such tasks frequently arise in genomics, web-scale data mining,
		machine learning, and other large-scale applications.
	\end{itemize}
\end{frame}

\begin{frame}{Group Testing}
	\begin{itemize}
		\item We are given a set $D$ of $N$ items, with $k$ positives (“hits”) and $N-k$ negatives (“misses”).
		\item \textbf{Goal:} Identify all positive items using fewer than $N$ group tests.
		\item A \textit{group test} is positive iff at least one item in the group is positive.
		\item \textbf{Testing Variants:} Can be \textit{noisy} (with false positives/negatives), \textit{adaptive} (tests depend on previous results), or \textit{non-adaptive} (all tests run in parallel).
		\item The paper uses a \textbf{doubly regular design:} Each item appears in an equal number of tests; each test has an equal number of items.
	\end{itemize}
\end{frame}

\begin{frame}{Formal Problem Statement}
	\begin{itemize}
		\item \textbf{(R, c)-Approximate Near Neighbor:} Given a dataset $D$, if there exists a point within distance $R$ of a query $y$, return some point within distance $c \cdot R$, with high probability.
		\begin{itemize}
			\item $R$ is the distance threshold (radius).
			\item $c > 1$ is the approximation factor.
		\end{itemize}
		\item Any algorithm that solves the randomized nearest neighbor problem also solves the approximate near neighbor problem with $c=1$ and any $R \ge$ distance to the nearest neighbor.
		\item {(Definition)}
		{\bf Randomized Nearest neighbor:} Given a dataset $D$ and a distance metric $d(\cdot, \cdot)$ and a failure probability $\delta \in [0, 1]$, construct a data structure which, given a query point $y$ reports the point $x \in D$ with the smallest distance $d(x,y)$ with probability greater than $1 - \delta$.
	\end{itemize}
\end{frame}

\section{Locality Sensitive Hashing}

\begin{frame}{Locality Sensitive Hashing}
	A hash function $h(x) \rightarrow \{1, \dots, R\}$ is a function that maps an input $x$ to an integer in the range $[1, R]$. 

	The two points $x$ and $y$ are said to collide if $h(x) = h(y)$.

	\[s(x, y) = Pr_H(h(x) = h(y))\]

	For now, we will assume that $s(x, y) = sim(x, y)$.

	For any positive integer L, we may transform an LSH family $H$ with collision probability $s(x,y)$ into a new family having $s(x,y)^L$ by sampling $L$ hash functions from $H$ and concatenating the values to obtain a new hash code $[h_1(x),h_2(x),...,h_L(x)]$. If the original hash family had the range $[1, R]$, the new hash family has the range $[1, R^L]$.
\end{frame}
\begin{frame}{Locality Sensitive Hashing}
	\begin{itemize}
		\item {\bf Locality Sensitive Hashing (LSH)} algorithms use an LSH function to partition the dataset into buckets.
		\item The hash function is selected so that the distance between points in the same bucket is likely to be small. 
		\item To find the near neighbors of a query, we hash the query and compute the distance to every point in the corresponding bucket.
		\item  {\bf Count-Based LSH} identifies neighbors by simply counting how many times two points land in the same hash bucket across multiple hash functions.
	\end{itemize}
\end{frame}





\section{Distance-Sensitive Bloom Filters}
\begin{frame}{Distance-Sensitive Bloom Filters}
	\begin{itemize}
		\item (Definition) {\bf Approximate Set Membership:} Given a set $D$ of $N$ points and similarity thresholds
		$S_L$ and $S_H$, construct a data structure which, given a query point $y$, has:

		True Positive Rate: If there is $x \in D$ with $sim(x,y) >S_H$, the structure returns true w.p. $ \geq p$

		False Positive Rate: If there is no $x \in D$ with $sim(x,y) > S_L$, the structure returns true w.p. $\leq q$

	\item The distance-sensitive Bloom filter solves this problem using LSH functions and a 2D bit array. The structure consists of $m$ binary arrays that are each indexed by an LSH function. There are threeparameters: the number of arrays $m$, a positive threshold $t \leq m$, and the number of concatenated hash functions $L$ used within each array.  
	\end{itemize}
\end{frame}

\begin{frame}{Distance-Sensitive Bloom Filters}
	\begin{itemize}
		\item To construct the filter, we insert elements $x \in D$ by setting the bit located at array index $[m,h_m(x)]$ to 1. 
		\item To query the filter, we determine the $m$ hash values of the query $y$. If at least $t$ of the corresponding
		bits are set, we return true. Otherwise, we return false. 
		\item (Theorem) Assuming the existence of an LSH family with collision probability $s(x,y) = sim(x,y)$,
		the distance-sensitive Bloom filter solves the approximate membership query problem with
		\[p \geq 1 - exp\left(-2m(-t + S_H^L)^2\right)\]
		\[q \leq exp\left(-2m(-t + NS_L^L)^2\right)\]
	\end{itemize}
\end{frame}





\end{document}


